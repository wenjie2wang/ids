{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4eb337f",
   "metadata": {},
   "source": [
    "<!-- (about_py)= -->\n",
    "\n",
    "# Naive Bayes Classification Problem:\n",
    "\n",
    "Classification problem is a supervised learning procedure to be able\n",
    "to label all data points as being part of a class. A bayesian\n",
    "perspecitive to classification problem is that we can calculate\n",
    "posterior probabilities of being from different classes given the data\n",
    "and assuming a prior. \n",
    "\n",
    "The reason it is called Naive is because it considers naively the\n",
    "assumption of conditional independence between features given the labels.\n",
    "\n",
    "\n",
    "Naive Bayes is a kind of supervised learning procedure as we consider\n",
    "a training dataset on which the model learns relation between features\n",
    "and label using a probabilistic model and then assigns labels using\n",
    "the model.\n",
    "\n",
    "\n",
    "Often when the label or the class is given, we can find the\n",
    "distribution of features, however, in Bayes method, we try to use the\n",
    "Bayes theorem to find the reverse probabilities, i.e., probability of\n",
    "label given the features.\n",
    "\n",
    "**Bayes Theorem:**\n",
    "$$P(Label|X_1, X_2,..X_n) = \\frac{\\prod{P(X_{i}|Label)}.\n",
    "P(Label)}{P(X_1, X_2, ... X_n)}$$\n",
    "\n",
    "We often term $P(X_1, X_2,..X_n|Label)$ as the likelihood, $P(Label)$ asthe\n",
    "prior probability of class, $P(X_1, X_2,..X_n)$ as the predictor prior\n",
    "probability and lastly, $P(Label|X_1, X_2,..X_n)$ as the posterior\n",
    "probability.\n",
    "\n",
    "If the probability of a given label is more\n",
    "than the others, we choose that label for a given object.  Sometimes,\n",
    "we can also look at the ratio of probabilities of various labels to\n",
    "see if it exceeds 1 or similarly look at the proportionalities using\n",
    "only the numerator. The reason to look at ratio or proportionalities is to avoid\n",
    "calculations of $P(X_1, X_2,..X_n)$\n",
    "\n",
    "**Kinds of Bayes Procedures:**\n",
    "\n",
    "There are three types of Naive Bayes Classifiers in `Scikit-learn` based\n",
    "on likelihood kernel for the features given label:\n",
    "\n",
    "- `GaussianNB` - likelihood is assumed to be normal\n",
    "- `MultinomialNB` - used for multinomial data, especially used in text\n",
    "  classification and is characterized by $\\theta_y = (\\theta_{1y},\n",
    "  \\theta_{2y}, ... \\theta_{ny})$\n",
    "- `ComplementNB` - An alternate to `MultinomialNB` in imbalanced\n",
    "  datasets. (Often outperforms `Multinomial` in text classification)\n",
    "- `BernoulliNB` - Assumes each feature is binary-valued.\n",
    "- `CategoricalNB` - Assumes each feature has its own categorical\n",
    "  distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e034cf0c",
   "metadata": {},
   "source": [
    "## Toy Example\n",
    "\n",
    "- We first load the dataset from sklearn.datasets. \n",
    "- Note the `dir(data)` suggests that it has data, target which attribute to X and y.\n",
    "- This suggests the pattern for kind of dataset that is accepted for naive bayes fit in `Sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b739b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn\n",
    "import sklearn\n",
    "\n",
    "#Loading the Dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "data = load_breast_cancer()\n",
    "print(type(data))\n",
    "print(dir(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fe1dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.feature_names)\n",
    "print(data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3652e3f2",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "- Using test_size, and random_state, we split the data (X) and target (Y) into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide into train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size = 0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966003b1",
   "metadata": {},
   "source": [
    "### Fitting the Training Data to Gaussian NB\n",
    "\n",
    "- We fit using `.fit` method to training data.\n",
    "- We can access probabilities using `predict_proba` method, but according to Scikit learn page it is found: \"although Naive Bayes is a decent classifier, it is a bad estimator and thus probabilities are not to be taken seriously\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1002f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    " \n",
    "#Calling the Class\n",
    "model = GaussianNB()\n",
    " \n",
    "#Fitting the data to the classifier\n",
    "model.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c06c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to access probabilities for X_train or X_test, use the following:\n",
    "## Note that the following output is an ndarray and taking the first 6 rows. \n",
    "model.predict_proba(X_test)[1:6,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45265835",
   "metadata": {},
   "source": [
    "### Predicting based on the fit on X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe7a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred = model.predict(X_test)   ## Predicted Probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009567f",
   "metadata": {},
   "source": [
    "### Metrics:\n",
    "\n",
    "- For multiclass, there are more options especially the kind of averaging. If you want to know more use `help(metrics.f1_score)`\n",
    "- I am doing precision, recall and f1_score amongst the many metrics that it has.\n",
    "- More metrics can be found at: [https://scikit-learn.org/stable/modules/model_evaluation.html](https://scikit-learn.org/stable/modules/model_evaluation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf801e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(f\"Precision: {metrics.precision_score(y_test, model_pred)}\")\n",
    "print(f\"Recall: {metrics.recall_score(y_test, model_pred)}\")\n",
    "print(f\"F1 Score: {metrics.f1_score(y_test, model_pred)}\")\n",
    "print(f\"AUC: {metrics.roc_auc_score(y_test, model_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d93d8e8",
   "metadata": {},
   "source": [
    "## Credit Card Example \n",
    "\n",
    "- I assume that the dataset for credit card (150 MB) is stored under data locally as `creditcard.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77140ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "## Import Dataset\n",
    "data = pd.read_csv ('../data/creditcard.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eafce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encodes categorical text into understandable labels for machine learning (not really useful in this example)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoded_data = data.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a089d1",
   "metadata": {},
   "source": [
    "### Test/Train Split\n",
    "\n",
    "- One can also split based on time variable to ensure that test dataset consists of recent data.\n",
    "- However, here we're using the sklearn's function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide into train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_data.drop(['Class'], axis = 1), encoded_data['Class'], test_size = 0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5f8152",
   "metadata": {},
   "source": [
    "### Fitting on Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116de3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calling the Class\n",
    "model = GaussianNB()\n",
    " \n",
    "#Fitting the data to the classifier\n",
    "model.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffdb65c",
   "metadata": {},
   "source": [
    "### Predicting class for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae9770",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred = model.predict(X_test)   ## Predicted Probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640857f",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b82e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(f\"Precision: {metrics.precision_score(y_test, model_pred)}\")\n",
    "print(f\"Recall: {metrics.recall_score(y_test, model_pred)}\")\n",
    "print(f\"F1 Score: {metrics.f1_score(y_test, model_pred)}\")\n",
    "print(f\"AUC: {metrics.roc_auc_score(y_test, model_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bede805",
   "metadata": {},
   "source": [
    "## Text documents\n",
    "\n",
    "We can also work on text documents especially using `MultinomialNB` to determine and classify text. This can be used in identifying SPAM emails etc. \n",
    "\n",
    "I am considering the example noted in: [Scikit Learn Website](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ddd2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c2e5ae",
   "metadata": {},
   "source": [
    "We download the dataset called 20newsgroups and consider all text corresponding to four categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7036a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Target: {twenty_train.target_names}\")\n",
    "print(f\"data: {len(twenty_train.data)}\")\n",
    "print(f\"Type of training data: {type(twenty_train)}\")\n",
    "print(f\"Type of data component: {type(twenty_train.data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a8492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.data[1]  ## just to give an idea of the data. (It is a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e198721",
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2388b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in twenty_train.target[:10]:\n",
    "    print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d65af8",
   "metadata": {},
   "source": [
    "## Using Bag of Words\n",
    "\n",
    "- We use the words in each text in training dataset and construct a dictionary mapped to integer indices.\n",
    "- For each text, we can count the instance of words and let that determine using NB method.\n",
    "\n",
    "We use `CountVectorizer` and CountVectorizer supports counts of N-grams of words or consecutive characters. Once fitted, the vectorizer has built a dictionary of feature indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ec5d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Occurence count\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e89a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts[:10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7bfe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Frequency count\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d135306b",
   "metadata": {},
   "source": [
    "## Training a Classifier (NB method)\n",
    "\n",
    "Now that we have the frequency data in a sparse data, and also our targets, we can train a classifier using NB method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31575c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "X_new_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e7c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_new_tfidf)\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "     print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98076b2",
   "metadata": {},
   "source": [
    "## Building a Pipeline\n",
    "\n",
    "In order to make the vectorizer => transformer => classifier easier to work with, scikit-learn provides a Pipeline class that behaves like a compound classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f3a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf.fit(twenty_train.data, twenty_train.target) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c69856",
   "metadata": {},
   "source": [
    "## Predicting and checking metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f926da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f618dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   70,
   78,
   91,
   94,
   99,
   104,
   111,
   121,
   125,
   129,
   131,
   139,
   146,
   152,
   159,
   163,
   170,
   175,
   179,
   185,
   189,
   191,
   195,
   202,
   210,
   215,
   219,
   226,
   230,
   234,
   237,
   246,
   255,
   259,
   266,
   272,
   277,
   284,
   288,
   294,
   303,
   305,
   309,
   317
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}