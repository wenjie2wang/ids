{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35960432",
   "metadata": {},
   "source": [
    "# Measures of classification accuracy and functions in Python\n",
    "\n",
    "source: [Machine Learning Mastery](https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/)\n",
    "\n",
    "Classification accuracy is the total number of correct predictions divided by the  \n",
    "total number of predictions made for a dataset.  \n",
    "\n",
    "As a performance measure, accuracy is inappropriate for imbalanced classification problems.  \n",
    "\n",
    "The main reason is that the overwhelming number of examples from the majority class (or classes)  \n",
    "will overwhelm the number of examples in the minority class, meaning that even unskillful models  \n",
    "can achieve accuracy scores of 90 percent, or 99 percent, depending on how severe  \n",
    "the class imbalance happens to be.  \n",
    "\n",
    "An alternative to using classification accuracy is to use **precision** and **recall** metrics.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "\\* **Precision** quantifies the number of positive class predictions that actually  \n",
    "belong to the positive class.   \n",
    "\n",
    "\\* **Recall** quantifies the number of positive class predictions made out of all   \n",
    "positive examples in the dataset.  \n",
    "\n",
    "\\* **F-Measure** provides a single score that balances both the concerns of precision  \n",
    "and recall in one number.\n",
    "\n",
    "## Confusion Matrix for Imbalanced Classification\n",
    "\n",
    "Before we dive into precision and recall, it is important to review the [confusion matrix](https://machinelearningmastery.com/confusion-matrix-machine-learning/).  \n",
    "\n",
    "For classification problems, the majority class is typically referred to as the  \n",
    "negative outcome (e.g. such as “no change” or “negative test result“), and the  \n",
    "minority class is typically referred to as the positive outcome (e.g. “change”   \n",
    "or “positive test result”).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The confusion matrix provides more insight into not only the performance  \n",
    "of a predictive model, but also which classes are being predicted correctly,  \n",
    "which incorrectly, and what type of errors are being made.  \n",
    "\n",
    "The **simplest confusion matrix** is for a two-class classification problem, with  \n",
    "**negative (class 0) and positive (class 1) classes**.  \n",
    "\n",
    "In this type of confusion matrix, each cell in the table has a specific and   \n",
    "well-understood name, summarized as follows:  \n",
    "\n",
    "\n",
    "|               | Positive Prediction | Negative Prediction|\n",
    "| ------------- | ------------------- | ------------------ |\n",
    "|Positive Class | True Positive (TP)  | False Negative (FN)|\n",
    "|Negative Class | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "The **precision and recall metrics** are defined in terms of the cells in  \n",
    "the confusion matrix, specifically terms like true positives and false negatives.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/525px-Precisionrecall.svg.png\" alt=\"Precision and Recall\" width=\"400\" height=\"400\" /> \n",
    "(Image by Wikipedia)  \n",
    "\n",
    "## Precision metric\n",
    "\n",
    "**Precision** is a metric that quantifies the number of correct positive predictions made.  \n",
    "\n",
    "**Precision**, therefore, calculates the accuracy for the **minority** class.\n",
    "\n",
    "It is calculated as the **ratio of correctly predicted positive examples divided by the  \n",
    "total number of positive examples that were predicted**.\n",
    "\n",
    "### Precision for Binary Classification\n",
    "\n",
    "In an imbalanced classification problem with two classes,   \n",
    "**precision** is calculated as **the number of true positives divided by  \n",
    "the total number of true positives and false positives**.  \n",
    "\n",
    "\\* **Precision = TruePositives / (TruePositives + FalsePositives)**  \n",
    "\n",
    "The result is a value between 0.0 for no precision and 1.0 for full or perfect precision.\n",
    "\n",
    "A model makes predictions and predicts 120 examples as belonging  \n",
    "to the minority class, 90 of which are correct, and 30 of which are incorrect.  \n",
    "\n",
    "The precision for this model is calculated as:  \n",
    "\n",
    "Precision = TruePositives / (TruePositives + FalsePositives)  \n",
    "Precision = 90 / (90 + 30)  \n",
    "Precision = 90 / 120  \n",
    "Precision = 0.75  \n",
    "\n",
    "The result is a precision of 0.75, which is a reasonable value but not outstanding.  \n",
    "\n",
    "You can see that precision is simply the ratio of correct positive predictions out  \n",
    "of all positive predictions made, or the accuracy of minority class predictions.  \n",
    "\n",
    "Consider the same dataset, where a model predicts 50 examples belonging to the minority   \n",
    "class, 45 of which are true positives and five of which are false positives. We can  \n",
    "calculate the precision for this model as follows:  \n",
    "\n",
    "Precision = TruePositives / (TruePositives + FalsePositives)  \n",
    "Precision = 45 / (45 + 5)  \n",
    "Precision = 45 / 50  \n",
    "Precision = 0.90  \n",
    "In this case, although the model predicted far fewer examples as belonging to the  \n",
    "minority class, the ratio of correct positive examples is much better.  \n",
    "\n",
    "This highlights that although precision is useful, it does not tell the whole story.   \n",
    "It does not comment on how many real positive class examples were predicted as belonging  \n",
    "to the negative class, so-called false negatives.\n",
    "\n",
    "###  Calculate Precision With Scikit-Learn\n",
    "\n",
    "The precision score can be calculated using the [precision_score() scikit-learn function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html).  \n",
    "\n",
    "For example, we can use this function to calculate precision for the scenarios in the previous section.  \n",
    "\n",
    "First, the case where there are 100 positive to 10,000 negative examples, and a model   \n",
    "predicts 90 true positives and 30 false positives. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9600a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates precision for 1:100 dataset with 90 tp and 30 fp\n",
    "# pip install -U scikit-learn\n",
    "from sklearn.metrics import precision_score\n",
    "# define actual\n",
    "act_pos = [1 for _ in range(100)]\n",
    "act_neg = [0 for _ in range(10000)]\n",
    "y_true = act_pos + act_neg\n",
    "# define predictions\n",
    "pred_pos = [0 for _ in range(10)] + [1 for _ in range(90)]\n",
    "pred_neg = [1 for _ in range(30)] + [0 for _ in range(9970)]\n",
    "y_pred = pred_pos + pred_neg\n",
    "# calculate prediction\n",
    "precision = precision_score(y_true, y_pred, average='binary')\n",
    "print('Precision: %.3f' % precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab30c1",
   "metadata": {},
   "source": [
    "## Recall Metric\n",
    "\n",
    "**Recall** is a metric that quantifies **the number of correct positive predictions   \n",
    "made out of all positive predictions that could have been made**.  \n",
    "\n",
    "Unlike precision that only comments on the correct positive predictions out of  \n",
    "all positive predictions, recall provides an indication of missed positive predictions.  \n",
    "\n",
    "In this way, **recall** provides some **notion of the coverage of the positive class**.\n",
    "\n",
    "### Recall for Binary Classification\n",
    "\n",
    "In an imbalanced classification problem with two classes, **recall**  \n",
    "is calculated as **the number of true positives divided by the total  \n",
    "number of true positives and false negatives**.\n",
    "\n",
    "**Recall = TruePositives / (TruePositives + FalseNegatives)**  \n",
    "The result is a value between 0.0 for no recall and 1.0 for full or perfect recall.  \n",
    "\n",
    "Let’s make this calculation concrete with some examples.  \n",
    "\n",
    "As in the previous section, consider a dataset with 1:100 minority to majority ratio,  \n",
    "with 100 minority examples and 10,000 majority class examples.  \n",
    "\n",
    "A model makes predictions and predicts 90 of the positive class predictions correctly  \n",
    "and 10 incorrectly. We can calculate the recall for this model as follows:  \n",
    "\n",
    "Recall = TruePositives / (TruePositives + FalseNegatives)  \n",
    "Recall = 90 / (90 + 10)  \n",
    "Recall = 90 / 100  \n",
    "Recall = 0.9  \n",
    "This model has a good recall.\n",
    "\n",
    "### Calculate Recall With Scikit-Learn\n",
    "\n",
    "The recall score can be calculated using the [recall_score() scikit-learn function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html).  \n",
    "\n",
    "For example, we can use this function to calculate recall for the scenarios above.\n",
    "\n",
    "First, we can consider the case of a 1:100 imbalance with 100 and 10,000 examples  \n",
    "respectively, and a model predicts 90 true positives and 10 false negatives.  \n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ef6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates recall for 1:100 dataset with 90 tp and 10 fn\n",
    "from sklearn.metrics import recall_score\n",
    "# define actual\n",
    "act_pos = [1 for _ in range(100)]\n",
    "act_neg = [0 for _ in range(10000)]\n",
    "y_true = act_pos + act_neg\n",
    "# define predictions\n",
    "pred_pos = [0 for _ in range(10)] + [1 for _ in range(90)]\n",
    "pred_neg = [0 for _ in range(10000)]\n",
    "y_pred = pred_pos + pred_neg\n",
    "# calculate recall\n",
    "recall = recall_score(y_true, y_pred, average='binary')\n",
    "print('Recall: %.3f' % recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ddb40",
   "metadata": {},
   "source": [
    "## Precision vs. Recall for Imbalanced Classification\n",
    "\n",
    "You may decide to use precision or recall on your imbalanced classification problem.\n",
    "\n",
    "Maximizing precision will minimize the number false positives, whereas maximizing  \n",
    "the recall will minimize the number of false negatives.  \n",
    "\n",
    "**Precision: Appropriate when minimizing false positives is the focus.  \n",
    "Recall: Appropriate when minimizing false negatives is the focus.**    \n",
    "Sometimes, we want excellent predictions of the positive class. We want high precision and high recall.  \n",
    "\n",
    "This can be challenging, as often increases in recall often come at the expense of decreases in precision.\n",
    "\n",
    "## F-Measure for Imbalanced Classification\n",
    "\n",
    "**Classification accuracy** is widely used because it is one single measure used  \n",
    "to summarize model performance.\n",
    "\n",
    "**F-Measure** provides a way to **combine both precision and recall** into a single measure  \n",
    "that captures both properties.  \n",
    "\n",
    "Alone, neither precision or recall tells the whole story. We can have excellent precision  \n",
    "with terrible recall, or alternately, terrible precision with excellent recall.  \n",
    "F-measure provides a way to express both concerns with a single score.  \n",
    "\n",
    "Once precision and recall have been calculated for a classification problem,  \n",
    "the two scores can be combined into the calculation of the F-Measure.  \n",
    "\n",
    "The traditional F measure is calculated as follows:  \n",
    "**F-Measure = (2 * Precision * Recall) / (Precision + Recall)**  \n",
    "\n",
    "This is the [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean) of the two fractions. This is sometimes called the  \n",
    "F-Score or the F1-Score and might be the most common metric used on imbalanced classification problems.\n",
    "\n",
    "A more general F score, $F_{\\beta }$,   \n",
    "that uses a positive real factor β, **where β is chosen such that recall is  \n",
    "considered β times as important as precision**, is:  \n",
    "$F_\\beta = (1 + \\beta^2) \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{(\\beta^2 \\cdot \\mathrm{precision}) + \\mathrm{recall}}$  \n",
    "The more generic $F_{\\beta }$ score applies additional weights,   \n",
    "valuing one of precision or recall more than the other.\n",
    "\n",
    "The highest possible value of an F-score is 1.0, indicating perfect precision and recall,  \n",
    "and the lowest possible value is 0, if either the precision or the recall is zero.\n",
    "\n",
    "Like precision and recall, a poor F-Measure score is 0.0 and a best or perfect F-Measure score is 1.0  \n",
    "\n",
    "For example, a perfect precision and recall score would result in a perfect F-Measure score:  \n",
    "\n",
    "F-Measure = (2 * Precision * Recall) / (Precision + Recall)  \n",
    "F-Measure = (2 * 1.0 * 1.0) / (1.0 + 1.0)  \n",
    "F-Measure = (2 * 1.0) / 2.0  \n",
    "F-Measure = 1.0\n",
    "\n",
    "Let’s make this calculation concrete with a worked example.  \n",
    "\n",
    "Consider a binary classification dataset with 1:100 minority to majority ratio,  \n",
    "with 100 minority examples and 10,000 majority class examples.  \n",
    "\n",
    "Consider a model that predicts 150 examples for the positive class, 95 are correct  \n",
    "(true positives), meaning five were missed (false negatives) and 55 are incorrect (false positives).  \n",
    "\n",
    "We can calculate the precision as follows:  \n",
    "\n",
    "Precision = TruePositives / (TruePositives + FalsePositives)  \n",
    "Precision = 95 / (95 + 55)  \n",
    "Precision = 0.633  \n",
    "We can calculate the recall as follows:  \n",
    "\n",
    "Recall = TruePositives / (TruePositives + FalseNegatives)  \n",
    "Recall = 95 / (95 + 5)  \n",
    "Recall = 0.95   \n",
    "This shows that the model has poor precision, but excellent recall.  \n",
    "\n",
    "Finally, we can calculate the F-Measure as follows:  \n",
    "\n",
    "F-Measure = (2 * Precision * Recall) / (Precision + Recall)  \n",
    "F-Measure = (2 * 0.633 * 0.95) / (0.633 + 0.95)  \n",
    "F-Measure = (2 * 0.601) / 1.583  \n",
    "F-Measure = 1.202 / 1.583  \n",
    "F-Measure = 0.759  \n",
    "We can see that the good recall levels-out the poor precision, giving an okay or reasonable F-measure score.\n",
    "\n",
    "### Calculate F-Measure With Scikit-Learn\n",
    "\n",
    "The F-measure score can be calculated using the f1_score() scikit-learn function.  \n",
    "\n",
    "For example, we use this function to calculate F-Measure for the scenario above.  \n",
    "\n",
    "This is the case of a 1:100 imbalance with 100 and 10,000 examples respectively,   \n",
    "and a model predicts 95 true positives, five false negatives, and 55 false positives.  \n",
    "  \n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates f1 for 1:100 dataset with 95tp, 5fn, 55fp\n",
    "from sklearn.metrics import f1_score\n",
    "# define actual\n",
    "act_pos = [1 for _ in range(100)]\n",
    "act_neg = [0 for _ in range(10000)]\n",
    "y_true = act_pos + act_neg\n",
    "# define predictions\n",
    "pred_pos = [0 for _ in range(5)] + [1 for _ in range(95)]\n",
    "pred_neg = [1 for _ in range(55)] + [0 for _ in range(9945)]\n",
    "y_pred = pred_pos + pred_neg\n",
    "# calculate score\n",
    "score = f1_score(y_true, y_pred, average='binary')\n",
    "print('F-Measure: %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210fe86c",
   "metadata": {},
   "source": [
    "## Extension\n",
    "\n",
    "sklearn.metrics.precision_score(y_true, y_pred, *, labels=None, pos_label=1, __average ='binary'__ , sample_weight=None, zero_division='warn')\n",
    "\n",
    "'binary':\n",
    "Only report results for the class specified by pos_label.   \n",
    "This is applicable only if targets ($y_{true,pred}$) are binary.  \n",
    "\n",
    "'micro':\n",
    "Calculate metrics globally by counting the total true positives,  \n",
    "false negatives and false positives.  \n",
    "\n",
    "'macro':\n",
    "Calculate metrics for each label, and find their unweighted mean.  \n",
    "This does not take label imbalance into account.  \n",
    "\n",
    "'weighted':\n",
    "Calculate metrics for each label, and find their average weighted by support   \n",
    "(the number of true instances for each label).  \n",
    "This alters ‘macro’ to account for label imbalance;  \n",
    "it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "'samples':\n",
    "Calculate metrics for each instance, and find their average (only meaningful  \n",
    "for multilabel classification where this differs from accuracy_score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dae7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]\n",
    "precision_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e85f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e2614",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3448d7",
   "metadata": {},
   "source": [
    "More resource: [Multi-Class Metrics](https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad05cb35",
   "metadata": {},
   "source": [
    "## ROC Curves and ROC AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c169d42",
   "metadata": {},
   "source": [
    "An ROC curve (or receiver operating characteristic curve) is a plot that summarizes the performance of a      \n",
    " binary classification model on the positive class.\n",
    "\n",
    "The x-axis indicates the False Positive Rate and the y-axis indicates the True Positive Rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ceb909",
   "metadata": {},
   "source": [
    "- ROC Curve: Plot of False Positive Rate (x) vs. True Positive Rate (y).  \n",
    "- TruePositiveRate = TruePositives / (TruePositives + False Negatives)  \n",
    "- FalsePositiveRate = FalsePositives / (FalsePositives + TrueNegatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da0718c",
   "metadata": {},
   "source": [
    "Ideally, we want the fraction of correct positive class predictions to be 1 (top of the plot) and the  \n",
    "fraction of incorrect negative class predictions to be 0 (left of the plot). This highlights that the  \n",
    "best possible classifier that achieves perfect skill is the top-left of the plot (coordinate 0,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497870da",
   "metadata": {},
   "source": [
    "We can plot a ROC curve for a model in Python using the [roc_curve() scikit-learn function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html).\n",
    "\n",
    "The function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1  \n",
    "class. The function returns the false positive rates for each threshold, true positive rates for each  \n",
    " threshold and thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e8cea5",
   "metadata": {},
   "source": [
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35387ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a roc curve for a predictive model\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from matplotlib import pyplot\n",
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# fit a model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "yhat = model.predict_proba(testX)\n",
    "# retrieve just the probabilities for the positive class\n",
    "pos_probs = yhat[:, 1]\n",
    "# plot no skill roc curve\n",
    "pyplot.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n",
    "# calculate roc curve for model\n",
    "fpr, tpr, _ = roc_curve(testy, pos_probs)\n",
    "# plot model roc curve\n",
    "pyplot.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45771ab1",
   "metadata": {},
   "source": [
    "The ROC Curve for the Logistic Regression model is shown (orange with dots). A no skill classifier as a  \n",
    " diagonal line (blue with dashes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71d3717",
   "metadata": {},
   "source": [
    "### ROC Area Under Curve (AUC) Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960b823a",
   "metadata": {},
   "source": [
    "Although the ROC Curve is a helpful diagnostic tool, it can be challenging to compare two or more   \n",
    "classifiers based on their curves.  \n",
    "\n",
    "Instead, the area under the curve can be calculated to give a single score for a classifier model across  \n",
    " all threshold values. This is called the ROC area under curve or ROC AUC or sometimes ROCAUC.     \n",
    "\n",
    "The score is a value between 0.0 and 1.0 for a perfect classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1acde",
   "metadata": {},
   "source": [
    "The AUC for the ROC can be calculated in scikit-learn using the [roc_auc_score() function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html).  \n",
    "\n",
    "Like the roc_curve() function, the AUC function takes both the true outcomes (0,1) from the test set and the  \n",
    " predicted probabilities for the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54621bb",
   "metadata": {},
   "source": [
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80610b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a roc auc for a predictive model\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# no skill model, stratified random class predictions\n",
    "model = DummyClassifier(strategy='stratified')\n",
    "model.fit(trainX, trainy)\n",
    "yhat = model.predict_proba(testX)\n",
    "pos_probs = yhat[:, 1]\n",
    "# calculate roc auc\n",
    "roc_auc = roc_auc_score(testy, pos_probs)\n",
    "print('No Skill ROC AUC %.3f' % roc_auc)\n",
    "# skilled model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(trainX, trainy)\n",
    "yhat = model.predict_proba(testX)\n",
    "pos_probs = yhat[:, 1]\n",
    "# calculate roc auc\n",
    "roc_auc = roc_auc_score(testy, pos_probs)\n",
    "print('Logistic ROC AUC %.3f' % roc_auc)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.13.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   133,
   148,
   194,
   208,
   303,
   317,
   345,
   352,
   356,
   358,
   362,
   366,
   373,
   379,
   385,
   393,
   397,
   428,
   433,
   437,
   447,
   454,
   458
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}