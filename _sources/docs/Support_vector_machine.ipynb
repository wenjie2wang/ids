{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ab6600",
   "metadata": {},
   "source": [
    "# Support vector machine\n",
    "\n",
    "## SVM in python with `sklearn`\n",
    "Take the following simulaiton data as an example:\n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn import svm\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# simulation data\n",
    "from sklearn.datasets import make_blobs\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "# make_blobs & make_classification can be used to generate multiclass simulation data\n",
    "X, y = make_blobs(n_samples=100, n_features=2, centers=3, random_state=123)\n",
    "plt.scatter(X[:,0],X[:,1],c=y)\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# Fitting a SVM\n",
    "model_svc = svm.SVC(kernel='linear') # svc = \"Support vector classifier\"\n",
    "model_svc.fit(X,y)\n",
    "# The key is the \"support vector\"\n",
    "model_svc.support_vectors_\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# Create a mesh to plot\n",
    "h = .02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = model_svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.scatter(X[:,0],X[:,1],c=y)\n",
    "```\n",
    "\n",
    "## Linear SVM\n",
    "Suppose the given data is $(\\vec X,y)=(\\vec x_1,y_1),...,(\\vec x_n,y_n)$ and $y_i$ are 1 or -1, indicating different classes.\\\n",
    "Any hyperplane($w$ is the normal vector to the plane) with the form:\n",
    "\n",
    "$$\n",
    "w^Tx-b=0\n",
    "$$\n",
    "\n",
    "You can use these boundaries to separate the data:\n",
    "\n",
    "$$\n",
    " w^Tx-b&=1\\\\\n",
    " w^Tx-b&=-1\n",
    "$$\n",
    "\n",
    "Anything above first boundary is label 1, and anything below second boundary is label 2.\n",
    "Geometrically we want to maximize the distance between the boundaries, which is $\\frac{2}{||w||}$.\\\n",
    "We also want to prevent points from falling into the margin, so we add the constraints:\n",
    "$w^Tx_i-b\\ge1, y_i=1 \\quad w^Tx_i-b\\le-1, y_i=-1$\\\n",
    "The optimization for linear SVM is:\n",
    "\n",
    "$$\n",
    "\\min_w ||w||^2  \\\\\n",
    "\\text{s.t  } y_i(w^Tx_i-b)\\ge 1\n",
    "$$\n",
    "\n",
    "The loss function is(known as hard-margin):\n",
    "\n",
    "$$\n",
    "||w||^2+\\frac{\\lambda}{2}\\sum_i{[1-y_i(w^Tx_i-b)]}\n",
    "$$\n",
    "\n",
    "Also we can extend the linear SVM to cases when data are not linearly separable. The loss function becomes(known as soft-margin):\n",
    "\n",
    "$$\n",
    "||w||^2+\\frac{\\lambda}{2}\\sum_i{[1-y_i(w^Tx_i-b)]_+}\n",
    "$$\n",
    "\n",
    "The lagrangian dual of the above is:\n",
    "\n",
    "$$\n",
    "\\max f(c_1,...,c_n)=\\sum_i{c_i}-\\frac{1}{2}\\sum_i\\sum_j {y_ic_i(x_i^Tx_j)y_jc_j}\\\\\n",
    "\\text{s.t  } \\sum_i{c_iy_i}=0, 0\\le c_i\\le1/2n\\lambda\n",
    "$$\n",
    "\n",
    "## Kernel SVM\n",
    "Sometimes the data cannot be separate by a hyperplane, look at the following example:\n",
    "\n",
    "\n",
    "```python\n",
    "# toy example for a non-linear separable data\n",
    "from sklearn.datasets import make_circles\n",
    "X1, y1 = make_circles(100,factor=.3,noise=.1)\n",
    "plt.scatter(X1[:, 0], X1[:, 1], c=y1, s=50)\n",
    "```\n",
    "\n",
    "If we project the data into higher dimension such a separate might be possible. For example, $\\phi(x_1,x_2)=(x_1,x_2,z=x_1^2+x_2^2)$.\n",
    "\n",
    "\n",
    "```python\n",
    "# scatter plot for transformed data (x1,x2,z)\n",
    "z1 = np.array(X1 ** 2).sum(1)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X1[:,0], X1[:,1], z1, c=y1)\n",
    "\n",
    "```\n",
    "\n",
    "Suppose we want to find a transform function $\\phi(\\cdot)$ and the classification vector in the transformed space is\n",
    "\n",
    "$$\\sum{c_iy_i\\phi(x_i)}$$\n",
    "\n",
    "And the linear kernel in the linear SVM is replaced by the $\\phi$ kernel:$k(x_i,x_j)=\\langle\\phi(x_i),\\phi(x_j)\\rangle$\n",
    "The optimization problem becomes:\n",
    "\n",
    "$$\n",
    "\\max f(c_1,...,c_n)=\\sum_i{c_i}-\\frac{1}{2}\\sum_i\\sum_j {y_ic_ik(x_i,x_j)y_jc_j}\\\\\n",
    "\\text{s.t} \\sum_i{c_iy_i}=0, 0\\le c_i\\le1/2n\\lambda\n",
    "$$\n",
    "\n",
    "Check the different SVM methods with different decision functions [here](https://scikit-learn.org/stable/modules/svm.html#mathematical-formulation)!\n",
    "\n",
    "### Kernel\n",
    "Kernels are symmetric functions in the form $K(x,y)$\\\n",
    "Examples of kernels:\\\n",
    "Linear kernel: $K(x,y)=x^Ty$\\\n",
    "Gaussian kernel(RBF): $K(x,y)=e^{-\\frac{||x-y||^2}{2\\sigma^2}}=e^{-\\gamma||x-y||^2}$\\\n",
    "Laplacian kernel: $K(x,y)=e^{-\\alpha||x-y||}$\\\n",
    "In `sklearn`, we can apply kernelized SVM by changing linear kernel to RBF kernel easily.\n",
    "There are two parameters in RBF kernel: $c$ and $\\gamma$. They are related to the smooth of decision surface and influence of single training sample. A [gridsearch CV](https://scikit-learn.org/stable/modules/svm.html#kernel-functions) can be used to decide these parameters.\n",
    "\n",
    "\n",
    "```python\n",
    "clf = svm.SVC(kernel = 'rbf')\n",
    "clf.fit(X1,y1)\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# Create a mesh to plot\n",
    "h = .02\n",
    "x_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\n",
    "y_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.scatter(X1[:,0],X1[:,1],c=y1)\n",
    "```\n",
    "\n",
    "## More about `sklearn.svm`\n",
    "There are functions for fitting a SVM classification/regression model. \n",
    "### Classification\n",
    "- SVC: traing \"one vs one\" model for muticlassification.\n",
    "- NuSVC: similar to SVC\n",
    "- LinearSVC: faster, but does not accept parameter kernel; \"one vs rest\" for muticlassification\n",
    "\n",
    "### Regression\n",
    "- SVR\n",
    "- NuSVR\n",
    "- LinearSVR\n",
    "\n",
    "### Probability\n",
    "SVM does not dirrectly provide probability estimates, but it can be calculated using cross-validation.\n",
    "\n",
    "\n",
    "```python\n",
    "# Iris data simulation\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# load data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# scaling data is usually good for SVM, and model-fitting remains same under scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# search C and gamma\n",
    "# For an initial search, a logarithmic grid with basis 10 is often helpful. \n",
    "# Using a basis of 2, a finer tuning can be achieved but at a much higher cost.\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = {'C': C_range,\n",
    "              'gamma': gamma_range}\n",
    "cv = StratifiedShuffleSplit(n_splits=5,test_size=0.2, random_state=1)\n",
    "grid = GridSearchCV(svm.SVC(), param_grid=param_grid,cv=cv)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"C_range: %s \\ngamma_range: %s\"% (C_range, gamma_range))\n",
    "print(grid.best_params_)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# visuallization\n",
    "# only use 2 features and binary Y\n",
    "X_2d = X[:, :2]\n",
    "X_2d = X_2d[y > 0]\n",
    "y_2d = y[y > 0]\n",
    "y_2d -= 1\n",
    "X_2d = scaler.fit_transform(X_2d)\n",
    "\n",
    "# train classifiers on a smaller c/gamma sets\n",
    "C_2d_range = [1e-2, 1, 1e2]\n",
    "gamma_2d_range = [1e-1, 1, 1e1]\n",
    "classifiers = []\n",
    "for C in C_2d_range:\n",
    "    for gamma in gamma_2d_range:\n",
    "        clf = svm.SVC(C=C, gamma=gamma)\n",
    "        clf.fit(X_2d, y_2d)\n",
    "        classifiers.append((C, gamma, clf))\n",
    "        \n",
    "\n",
    "# draw the decision boundaries\n",
    "xx, yy = np.meshgrid(np.linspace(-3, 3, 200), np.linspace(-3, 3, 200))\n",
    "for (k, (C, gamma, clf)) in enumerate(classifiers):\n",
    "    # evaluate decision function in a grid\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # visualize decision function for these parameters\n",
    "    plt.subplot(len(C_2d_range), len(gamma_2d_range), k + 1)\n",
    "    plt.title(\"gamma=10^%d, C=10^%d\" % (np.log10(gamma), np.log10(C)),\n",
    "              size='small')\n",
    "\n",
    "    # visualize parameter's effect on decision function\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, cmap=plt.cm.RdBu_r,\n",
    "                edgecolors='k')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# draw the decision boundaries\n",
    "xx, yy = np.meshgrid(np.linspace(-3, 3, 200), np.linspace(-3, 3, 200))\n",
    "for (k, (C, gamma, clf)) in enumerate(classifiers):\n",
    "    # evaluate decision function in a grid\n",
    "    W = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    W = W.reshape(xx.shape)\n",
    "    # visualize decision function for these parameters\n",
    "    plt.subplot(len(C_2d_range), len(gamma_2d_range), k + 1)\n",
    "    plt.title(\"gamma=10^%d, C=10^%d\" % (np.log10(gamma), np.log10(C)),\n",
    "              size='small')\n",
    "\n",
    "    # visualize parameter's effect on decision function\n",
    "    plt.contourf(xx, yy, W, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, cmap=plt.cm.RdBu_r,\n",
    "                edgecolors='k')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "```\n",
    "\n",
    "### Suggestions about tuning SVM kernels\n",
    "- `Gamma` is the parameter of the Gaussian radial basis function. `C` is the parameter for the soft margin cost function, which controls the influence of each individual support vector.\n",
    "- The `gamma` parameter defines how far the influence of a single training example reaches, with low values meaning 'far' and high values meaning 'close'. When `gamma` is very small, the model is too constrained and cannot capture the complexity or \"shape\" of the data. \n",
    "- The `C` parameter trades off correct classification against the decision function's margin. A lower`C` will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy. In other words `C` behaves as a regularization parameter in the SVM.\n",
    "- In practice, a logarithmic grid from $10^{-3}$ to $10^3$ is usually sufficient. If the best parameters lie on the boundaries of the grid, it can be extended in that direction in a subsequent search.\n",
    "\n",
    "# Reference\n",
    "- [https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/auto_examples/svm/plot_svm_parameters_selection.html]\n",
    "- [https://scikit-learn.org/stable/modules/svm.html]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "source_map": [
   13
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}